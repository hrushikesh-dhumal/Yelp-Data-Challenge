{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#######################\n",
    "__version__ = \"1.0\"\n",
    "__date__ = \"2016-04-19\"\n",
    "__modified_by__ = \"Hrushikesh Dhumal\"\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED_VAL = 200;\n",
    "WORK_DIR = os.getcwd();\n",
    "YELP_DATA_CSV_DIR = os.path.join(WORK_DIR, \"data\", \"csv\")\n",
    "YELP_DATA_WORD_2_VEC_MODEL_DIR = os.path.join(WORK_DIR, \"data\", \"word2vec_model\")\n",
    "YELP_DATA_FEAT_ADD = os.path.join(WORK_DIR, \"data\", \"pos_neg\")\n",
    "YELP_DATA_WORD_2_VEC_MODEL_DIR_FEAT_ADD = os.path.join(WORK_DIR, \"data\", \"word2vec_model_feat_add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_sure_path_exists(YELP_DATA_WORD_2_VEC_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_filename = os.path.join(YELP_DATA_CSV_DIR, 'business_review_user_0_1Percent.csv')\n",
    "df_data = pd.read_csv(read_filename, engine='c', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"I've been incredibly weary when it comes to mechanics as I've had some bad experiences, but I'm convinced that Tony and his team at Oasis are one of the best in town.\\r\\n\\r\\nI've been taking my Jeep Liberty into them since I bought it in 2010 after being referred by a friend, and they have consistently gone above and beyond when it comes to customer service, fair pricing, and clear communication about the vehicle's problem and options for getting it fixed. There have been one or two occasions where I've even called them, explained the problem, and they were able to diagnose the problem over the phone and - because the issue was so minor - walk me through how to take care of it on my own.\\r\\n\\r\\nGreat company that I completely trust with my vehicle. I would most definitely recommend Oasis to anyone who needs a good mechanic.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.review_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem(s):\n",
    "    s = remove_numbers_in_string(s)\n",
    "    s = lowercase_remove_punctuation(s)\n",
    "    s = remove_stopwords(s)\n",
    "    token_list = nltk.word_tokenize(s)\n",
    "    #token_list = filter_out_more_stopwords(token_list)\n",
    "    token_list = stem_token_list(token_list)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ive', u'incred', u'weari', u'come', u'mechan', u'ive', u'bad', u'experi', u'im', u'convinc', u'toni', u'team', u'oasi', u'one', u'best', u'town', u'ive', u'take', u'jeep', u'liberti', u'sinc', u'bought', u'refer', u'friend', u'consist', u'gone', u'beyond', u'come', u'custom', u'servic', u'fair', u'price', u'clear', u'commun', u'vehicl', u'problem', u'option', u'get', u'fix', u'one', u'two', u'occas', u'ive', u'even', u'call', u'explain', u'problem', u'abl', u'diagnos', u'problem', u'phone', u'issu', u'minor', u'walk', u'take', u'care', u'great', u'compani', u'complet', u'trust', u'vehicl', u'would', u'definit', u'recommend', u'oasi', u'anyon', u'need', u'good', u'mechan']\n"
     ]
    }
   ],
   "source": [
    "print lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem(df_data.review_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem( raw_sentence))\n",
    "    \n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in df_data[\"review_text\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'place', u'great', u'allow', u'eat', u'like', u'much', u'littl', u'would', u'like']\n"
     ]
    }
   ],
   "source": [
    "print sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data['pos_words'] = ''\n",
    "df_data['neg_words'] = ''\n",
    "df_data['net_senti'] = ''\n",
    "df_data['review_senti'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POS_WORDS_FILE = os.path.join(YELP_DATA_FEAT_ADD, 'positive-words.txt')\n",
    "NEG_WORDS_FILE = os.path.join(YELP_DATA_FEAT_ADD, 'negative-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 positive words ['a+', 'abound', 'abounds', 'abundance', 'abundant'] \n",
      "First 5 negative words ['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable']\n",
      "Number of positive words 2006\n",
      "Number of negative words 4783\n",
      "Total number of words 6789\n"
     ]
    }
   ],
   "source": [
    "with open(POS_WORDS_FILE) as f:\n",
    "    pos_words1 = f.read().split()[213:]\n",
    "\n",
    "with open(NEG_WORDS_FILE) as f:\n",
    "    neg_words1 = f.read().split()[213:]    \n",
    "    \n",
    "print \"First 5 positive words %s \" % pos_words1[:5]\n",
    "print \"First 5 negative words %s\" % neg_words1[:5]\n",
    "\n",
    "print \"Number of positive words %d\" % len(pos_words1)\n",
    "\n",
    "print \"Number of negative words %d\" % len(neg_words1)\n",
    "\n",
    "all_words_with_sentiment = pos_words1 + neg_words1\n",
    "\n",
    "print \"Total number of words %d\" % len(all_words_with_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords(s):\n",
    "    s = remove_numbers_in_string(s)\n",
    "    s = lowercase_remove_punctuation(s)\n",
    "    s = remove_stopwords(s)\n",
    "    token_list = []\n",
    "    token_list.append(nltk.word_tokenize(s))\n",
    "    #token_list = filter_out_more_stopwords(token_list)\n",
    "    #token_list = stem_token_list(token_list)\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "2225\n",
      "2225\n",
      "[u'place', u'great', u'allows', u'eat', u'like', u'much', u'little', u'would', u'like', u'love', u'place', u'frequent', u'often', u'great', u'large', u'parties']\n"
     ]
    }
   ],
   "source": [
    "all_reviews = [] # Initialize an empty list of sentences for each review\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in df_data[\"review_text\"]:\n",
    "    all_reviews += lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords(review)\n",
    "\n",
    "print len(all_reviews)\n",
    "print len(df_data)\n",
    "print all_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:4: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for sentence in all_reviews:\n",
    "    num_positive = sum([r in pos_words1 for r in sentence])\n",
    "    num_negative = sum([r in neg_words1 for r in sentence])\n",
    "    net_sentiment = num_positive - num_negative\n",
    "     \n",
    "#   num_positive #adds number of positive words to the dataframe for each review\n",
    "#   num_negative #adds number of negative words to the dataframe for each review\n",
    "#   net_sentiment #adds total weight to the dataframe for each review\n",
    "    \n",
    "    \n",
    "    df_data.set_value(i, 'pos_words', num_positive)\n",
    "    df_data.set_value(i, 'neg_words', num_negative)\n",
    "    df_data.set_value(i, 'net_senti', net_sentiment)\n",
    "    \n",
    "\n",
    "    if(net_sentiment > 0):\n",
    "        #adds 1 to the dataframe for each review denoting a positive tendency of the review\n",
    "        df_data.set_value(i, 'review_senti', 1)\n",
    "    elif(net_sentiment < 0):\n",
    "        #adds -1 to the dataframe for each review denoting a negative tendency of the review\n",
    "         df_data.set_value(i, 'review_senti', -1)\n",
    "    else:\n",
    "        #adds 0 to the dataframe for each review denoting a neutral tendency of the review\n",
    "         df_data.set_value(i, 'review_senti', 0)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id                                business_categories  \\\n",
      "0  SK77jbqCpIv_lwwRkhq9_w                     [u'Mongolian', u'Restaurants']   \n",
      "1  oGii4C9XqsG5m3Q-V8LfgA                    [u'Auto Repair', u'Automotive']   \n",
      "2  uXu7hX_bTrr-48op0Ykkyw  [u'Burgers', u'Breakfast & Brunch', u'Restaura...   \n",
      "3  4uGHPY-OpJN08CabtTAvNg                        [u'French', u'Restaurants']   \n",
      "4  nDnbIOqoKSAG339g6D8OnA  [u'Active Life', u'Yoga', u'Fitness & Instruct...   \n",
      "5  c8XlQvVNwKhH9BOHEObtNQ        [u'American (Traditional)', u'Restaurants']   \n",
      "6  PXviRcHR1mqdH4vRc2LEAQ                       [u'Burgers', u'Restaurants']   \n",
      "7  bn2HqLj_XpPG9UE3mAo5cg  [u'Burgers', u'Breakfast & Brunch', u'American...   \n",
      "8  nm8-Bn7O3DUytb5RZw4V5A  [u'Food', u'Ice Cream & Frozen Yogurt', u'Fast...   \n",
      "9  1w6ii_Mb64da6cAxqxKF2A  [u'Hotels & Travel', u'Transportation', u'Airp...   \n",
      "\n",
      "                business_name  business_review_count  \\\n",
      "0               Genghis Grill                    257   \n",
      "1     Oasis Auto Center Tempe                     36   \n",
      "2          Biscuits 2 Burgers                    136   \n",
      "3  L'Atelier de Joël Robuchon                    783   \n",
      "4                Amazing Yoga                     37   \n",
      "5                Claim Jumper                    210   \n",
      "6                Bachi Burger                   2404   \n",
      "7               Divine Eatery                    145   \n",
      "8                    Culver's                     11   \n",
      "9     Super Shuttle Las Vegas                    153   \n",
      "\n",
      "                               business_full_address business_open  \\\n",
      "0          550 N Stephanie St\\r\\nHenderson, NV 89014          True   \n",
      "1  1835 E Guadalupe Rd Suite E-116\\r\\nTempe, AZ 8...          True   \n",
      "2  9700 W Tropicana Ave\\r\\nSpring Valley\\r\\nLas V...          True   \n",
      "3  MGM Grand Hotel and Casino\\r\\n3799 Las Vegas B...          True   \n",
      "4  730 Copeland St\\r\\nShadyside\\r\\nPittsburgh, PA...          True   \n",
      "5              1530 W Baseline Rd\\r\\nTempe, AZ 85283          True   \n",
      "6  470 E Windmill Ln\\r\\nSte 100\\r\\nSoutheast\\r\\nL...          True   \n",
      "7  7181 N Hualapai Way\\r\\nSte 115\\r\\nCentennial\\r...          True   \n",
      "8             4301 E Towne Blvd\\r\\nMadison, WI 53704          True   \n",
      "9                                Las Vegas, NV 89103          True   \n",
      "\n",
      "   business_stars business_type  business_latitude  business_longitude  \\\n",
      "0             3.5      business          36.060104         -115.045988   \n",
      "1             4.5      business          33.361664         -111.909623   \n",
      "2             4.0      business          36.100546         -115.304025   \n",
      "3             4.5      business          36.102590         -115.170541   \n",
      "4             3.5      business          40.451095          -79.934871   \n",
      "5             3.0      business          33.378926         -111.964813   \n",
      "6             4.0      business          36.042798         -115.153225   \n",
      "7             4.0      business          36.291457         -115.315807   \n",
      "8             3.0      business          43.125075          -89.308307   \n",
      "9             4.0      business          36.106893         -115.187134   \n",
      "\n",
      "       ...      user_type user_average_stars user_elite user_votes.cool  \\\n",
      "0      ...           user               2.89         []              20   \n",
      "1      ...           user               5.00         []               0   \n",
      "2      ...           user               3.24         []              12   \n",
      "3      ...           user               4.21         []               6   \n",
      "4      ...           user               4.71         []              16   \n",
      "5      ...           user               3.94         []              38   \n",
      "6      ...           user               4.00         []               0   \n",
      "7      ...           user               3.56         []               5   \n",
      "8      ...           user               2.64         []             142   \n",
      "9      ...           user               5.00         []               0   \n",
      "\n",
      "   user_votes.funny  user_votes.useful  pos_words neg_words net_senti  \\\n",
      "0                55                145          5         0         5   \n",
      "1                 0                  2         12         6         6   \n",
      "2                15                 36          4         0         4   \n",
      "3                 4                 52          6         6         0   \n",
      "4                11                 27          5         0         5   \n",
      "5                68                198          1         0         1   \n",
      "6                 0                  0          1         0         1   \n",
      "7                 4                 10          7         0         7   \n",
      "8               752                372          1         1         0   \n",
      "9                 0                  0          3         0         3   \n",
      "\n",
      "   review_senti  \n",
      "0             1  \n",
      "1             1  \n",
      "2             1  \n",
      "3             0  \n",
      "4             1  \n",
      "5             1  \n",
      "6             1  \n",
      "7             1  \n",
      "8             0  \n",
      "9             1  \n",
      "\n",
      "[10 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print df_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import gensim, logging\n",
    "from gensim.models import Word2Vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Loading existing model\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "model_file = os.path.join(YELP_DATA_WORD_2_VEC_MODEL_DIR, \"100features_10minwords_10context__0_1Percent\")\n",
    "print \"Training model...\"\n",
    "if not os.path.isfile(model_file):\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "    model.save(model_file)\n",
    "else:\n",
    "    print \"Loading existing model\"\n",
    "    model = Word2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"amazing delightful bad\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0.:\n",
    "           print \"Review %d of %d\" % (counter, len(reviews))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 2225\n",
      "Review 1000 of 2225\n",
      "Review 2000 of 2225\n",
      "Wall time: 772 ms\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in df_data[\"review_text\"]:\n",
    "    clean_train_reviews.append( lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem( review))\n",
    "\n",
    "%time trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b = np.array(df_data.review_stars.copy())\n",
    "# b[(b == 1) | (b == 2) | (b == 3)] = 0\n",
    "# b[(b == 4) | (b == 5)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique, counts = np.unique(b, return_counts=True)\n",
    "\n",
    "# print np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainDataVecs = np.column_stack((trainDataVecs, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDataVecs = np.column_stack((trainDataVecs, df_data.pos_words))\n",
    "trainDataVecs = np.column_stack((trainDataVecs, df_data.neg_words))\n",
    "trainDataVecs = np.column_stack((trainDataVecs, df_data.net_senti))\n",
    "trainDataVecs = np.column_stack((trainDataVecs, df_data.review_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225L, 104L)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_add_feature_matrix_file = os.path.join(YELP_DATA_WORD_2_VEC_MODEL_DIR_FEAT_ADD, \"word2vec_add_feature_matrix_0_1Percent.csv\")\n",
    "np.savetxt(word2vec_feature_matrix_file, trainDataVecs, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.genfromtxt(\n",
    "    word2vec_add_feature_matrix_file,           # file name\n",
    "    delimiter=',')           # column delimiter                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(trainDataVecs, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
